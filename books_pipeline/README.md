# Books Processing Pipeline

A complete pipeline for processing books from PDF to searchable embeddings.

## Pipeline Overview

```
CSV + PDFs → OCR → Text Cleaning → Chunking → Embeddings → FAISS Index → Database
```

## Quick Start

1. **Setup environment:**

   ```bash
   cd books_pipeline
   pip install -r requirements.txt
   ```

2. **Install system dependencies:**

   - Ubuntu/Debian: `sudo apt-get install tesseract-ocr poppler-utils ghostscript`
   - macOS: `brew install tesseract poppler ghostscript`

3. **Add your files:**

   - Place CSV file in `input/csv/`
   - Place PDF files in `input/pdfs/`

4. **Run pipeline:**
   ```bash
   python scripts/01_ocr_books.py      # OCR processing
   python scripts/02_clean_texts.py    # Text cleaning
   python scripts/03_embed_books.py    # Create embeddings
   python scripts/04_create_indexes.py # Build FAISS indexes
   python scripts/05_load_to_db.py     # Load to database
   ```

## Directory Structure

```
books_pipeline/
├── input/
│   ├── csv/           # Input CSV with book metadata
│   └── pdfs/          # Input PDF files
├── intermediate/
│   ├── ocr/           # OCR output
│   ├── cleaned/       # Cleaned text
│   └── embeddings/    # Embeddings and chunks
├── output/
│   ├── embeddings/    # Final embedding arrays
│   ├── ids/           # Metadata mappings
│   └── indexes/       # FAISS indexes
├── scripts/           # Processing scripts
├── logs/              # Processing logs
├── config.py          # Configuration
└── requirements.txt   # Dependencies
```

## Expected CSV Format

CSV should contain these columns:

- **Core**: `Title`, `Author`, `Importance`, `Topic`, `filename`, `ID`
- **Google Books**: `title`, `authors`, `publisher`, `publishedDate`, `description`, `pageCount`, `categories`, `averageRating`, `ratingsCount`, `imageLinks`, `language`, `previewLink`
- **Scholar**: `scholar_title`, `scholar_url`, `scholar_type`, `scholar_snippet`, `scholar_publication_info_summary`, `scholar_author_names`, `scholar_author_links`
- **Search**: `search_title`, `search_url`, `search_snippet`, `search_source`
- **Public**: `public_url`, `public_authors`, `public_title`
- **Generated by pipeline**: `embed_id_dict`, `embed_filename` (will be populated)

## Pipeline Output

The pipeline will populate:

- `embed_id_dict`: JSON mapping chunk_ids to text content
- `embed_filename`: Reference to the processed file

Example embed_id_dict:

```json
{
  "filename_book.json__page_[0]__chunk_0": "Chapter 1 text...",
  "filename_book.json__page_[0]__chunk_1": "More text...",
  "filename_book.json__page_[1]__chunk_2": "Next page text...",
  "filename_book.json__page_[1, 2, 3]__chunk_3": "Multi-page chunk text...",
  "filename_book.json__page_[5, 6]__chunk_4": "Cross-page content..."
}
```

## Next Steps

After setup, run the first script:

```bash
python scripts/01_ocr_books.py
```
