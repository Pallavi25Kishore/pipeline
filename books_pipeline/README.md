# Books Processing Pipeline

A complete pipeline for processing books from PDF to searchable embeddings.

## Pipeline Overview

```
CSV + PDFs → OCR → Text Cleaning → Chunking → Embeddings → FAISS Index → Database → Whoosh Index
```

## Quick Start

1. **Setup environment:**

   ```bash
   cd books_pipeline
   pip install -r requirements.txt
   pip install Whoosh  # For keyword search
   ```

2. **Install system dependencies:**

   - Ubuntu/Debian: `sudo apt-get install tesseract-ocr poppler-utils ghostscript`
   - macOS: `brew install tesseract poppler ghostscript`

3. **Add your files:**

   - Place CSV file in `input/csv/`
   - Place PDF files in `input/pdfs/` (ensure filename in csv matches the pdf filename)

4. **Set up database credentials:**
   ```bash
   # Create .env file in books_pipeline/
   echo "DB_NAME=books" > .env
   echo "DB_USER=your_username" >> .env
   echo "DB_PASSWORD=your_password" >> .env
   echo "DB_HOST=localhost" >> .env
   echo "DB_PORT=5432" >> .env

5. **Run pipeline:**
   ```bash
   # Step 1-5: Run pipeline scripts
   python3 scripts/ocr.py      # OCR processing
   python3 scripts/clean_txt.py    # Text cleaning
   python3 scripts/embed_books.py    # Create embeddings
   python3 scripts/create_faiss_indexes.py # Build FAISS indexes
   python3 scripts/create_whoosh_index.py # Build Whoosh index

   # Step 6: Set up Django database #skip if db and tablesalready created
   cd books_project
   python3 manage.py makemigrations #only if model is changed otherwise skip this step
   python3 manage.py migrate
   python3 manage.py createsuperuser          # Optional: create admin user

   # Step 7: Load data
   python3 manage.py load_books --dry-run     # optional : Test first
   cd books_project
   python3 manage.py load_books               # Load to db

   # Step 8: Start web server
   python3 manage.py runserver 0.0.0.0:8000  # External access
   # Visit: http://your-server-ip:8000/admin/
   ```

## Directory Structure

```
books_pipeline/
├── input/
│   ├── csv/           # Input CSV with book metadata
│   └── pdfs/          # Input PDF files
├── intermediate/
│   ├── ocr/           # OCR output
│   ├── cleaned/       # Cleaned text
│   └── embeddings/    # Embeddings and chunks
├── output/
│   ├── embeddings/    # Final embedding arrays
│   ├── ids/           # Metadata mappings
│   ├── indexes/       # FAISS indexes
│   └── whoosh/        # Whoosh BM25 indexes
├── books_project/     # Django project
│   ├── manage.py
│   └── APIapp/
│       └── management/
│           └── commands/
│               └── load_books.py
├── scripts/           # Processing scripts
├── logs/              # Processing logs
├── config.py          # Configuration
└── requirements.txt   # Dependencies
```

## Expected CSV Format

CSV should contain these columns:

- **Core**: `Title`, `Author`, `Importance`, `Topic`, `filename`, `ID`
- **Google Books**: `title`, `authors`, `publisher`, `publishedDate`, `description`, `pageCount`, `categories`, `averageRating`, `ratingsCount`, `imageLinks`, `language`, `previewLink`
- **Scholar**: `scholar_title`, `scholar_url`, `scholar_type`, `scholar_snippet`, `scholar_publication_info_summary`, `scholar_author_names`, `scholar_author_links`
- **Search**: `search_title`, `search_url`, `search_snippet`, `search_source`
- **Public**: `public_url`, `public_authors`, `public_title`
- **Generated by pipeline**: `embed_id_dict`, `embed_filename` (will be populated)

## Pipeline Output

The pipeline will populate:

- `embed_id_dict`: JSON mapping chunk_ids to text content
- `embed_filename`: Reference to the processed file

Example embed_id_dict:

```json
{
  "filename_book.json__page_[0]__chunk_0": "Chapter 1 text...",
  "filename_book.json__page_[0]__chunk_1": "More text...",
  "filename_book.json__page_[1]__chunk_2": "Next page text...",
  "filename_book.json__page_[1, 2, 3]__chunk_3": "Multi-page chunk text...",
  "filename_book.json__page_[5, 6]__chunk_4": "Cross-page content..."
}
```

FAISS Vector Search Files (output):

output/indexes/books_flat_ip_YYYY-MM-DD.index - FAISS vector index
output/indexes/books_hnsw_YYYY-MM-DD.index - FAISS vector index
output/ids/books_metadata_YYYY-MM-DD.pkl - Pickle file mapping vector positions to chunk metadata
output/embeddings/books_embeddings_YYYY-MM-DD.npy - Raw embedding arrays

Whoosh Keyword Search Files (output):

output/whoosh/books_whoosh_YYYY-MM-DD/ - Directory containing Whoosh BM25 index files
output/whoosh/index_metadata.json - Index creation metadata


